{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11677191,"sourceType":"datasetVersion","datasetId":7328938},{"sourceId":11677294,"sourceType":"datasetVersion","datasetId":7329014}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/student344/arxiv-topic-modeling?scriptVersionId=248046800\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Machine Learning Research Topic Modeling with BERTopic\n\nThis notebook demonstrates how to perform topic modeling on a dataset of Machine Learning research papers from arXiv using the BERTopic library. It covers data loading, model training (or loading a pre-trained model), topic visualization, and analysis.\n\n### NOTE\nSometimes the visualization outputs might be blank. This is likely an issue with rendering in the Kaggle environment, and is solved by simply running the code again. Also, to avoid rendering issues, make sure that your ad-blocking extension is disabled.  ","metadata":{}},{"cell_type":"markdown","source":"## **1. Installation of Libraries**\n","metadata":{}},{"cell_type":"markdown","source":"Some of the dependencies we use don't need to be installed when the notebook is run on Kaggle, because they are included in every Kaggle environment.\nOverview of dependencies:\n*   **`bertopic`**: The core package for BERTopic-based topic modeling.\n*   **`litellm`**: A package that simplifies LLM API calls by providing a single API client for any LLM provider (e.g. Gemini, Anthropic, OpenAI, AWS Bedrock, etc.)\n*   **`octis`**: Used for topic coherence and topic diversity evaluation metrics.\n*   **`sentence-transformers`**: Used for generating sentence embeddings, which are crucial for BERTopic's understanding of semantic meaning.\n*   **`scikit-learn`**: Provides machine learning tools, including CountVectorizer used here for text preprocessing.\n*   **`pandas`**: Used for data manipulation, data analysis, and working with DataFrames.\n*   **`torch`**: PyTorch is a deep learning framework, and we are using it to check for GPU hardware acceleration availability.\n*   **`kagglehub`**: Used to fetch data from Kaggle.","metadata":{}},{"cell_type":"code","source":"%pip install litellm bertopic scikit-learn kagglehub octis --quiet ","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-06-29T01:24:50.390427Z","iopub.execute_input":"2025-06-29T01:24:50.390762Z","iopub.status.idle":"2025-06-29T01:24:53.860339Z","shell.execute_reply.started":"2025-06-29T01:24:50.390736Z","shell.execute_reply":"2025-06-29T01:24:53.859078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Importing Libraries and Preparing the Dataset","metadata":{}},{"cell_type":"code","source":"from bertopic import BERTopic\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\nimport torch\nimport kagglehub\nimport numpy as np\n\ndataset = \"/kaggle/input/arxiv-ml-ai-052023-052025/arxiv_ml_ai_papers_last_2_years.csv\"\n\ntry:\n    df = pd.read_csv(dataset)\nexcept:\n    dataset = kagglehub.dataset_download('student344/arxiv-ml-ai-052023-052025', path=\"/kaggle/input/arxiv-ml-ai-052023-052025/arxiv_ml_ai_papers_last_2_years.csv\")\n    df = pd.read_csv(dataset)\n    \ndf[\"text\"] = df[\"title\"] + \" \" + df[\"summary\"]\nprint(\"The dataset has been parsed successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:24:53.862438Z","iopub.execute_input":"2025-06-29T01:24:53.862686Z","iopub.status.idle":"2025-06-29T01:24:54.118703Z","shell.execute_reply.started":"2025-06-29T01:24:53.862664Z","shell.execute_reply":"2025-06-29T01:24:54.117702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After importing the required libraries, we combine each paper's title and abstract into a single text column for preprocessing. This combined text serves as input for our embedding model. \n\nWe preserve most of the text, including stop words, since transformer-based embedding models require full contextual information to generate accurate embeddings. Light preprocessing is performed to remove escape characters, LaTeX code, URLs, and other noise. [As recommended by BERTopic's developers](https://maartengr.github.io/BERTopic/faq.html#how-do-i-remove-stop-words), any additional preprocessing steps are performed *after* generating the embeddings.\n\n","metadata":{}},{"cell_type":"code","source":"import re\n\ndef preprocess(text: str) -> str:\n    # Remove inline LaTeX math expressions: $...$\n    text = re.sub(r'\\$(.*?)\\$', '', text)\n    \n    # Remove display math\n    text = re.sub(r'\\$\\$(.*?)\\$\\$', '', text, flags=re.DOTALL)\n    text = re.sub(r'\\\\(.*?)(.*?)\\\\', '', text, flags=re.DOTALL)\n\n    # Remove common LaTeX commands (e.g., \"\\cite{}\", \"?????????\\ref{}\", etc.)\n    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)\n    # Remove LaTeX escape sequences such as \\\\% or \\\\_\n    text = re.sub(r'\\\\([%_&#$])', r'\\1', text)\n\n    # Remove multiple spaces and newlines\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Remove URLs (http/https)\n    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n\n    # Strip leading/trailing whitespace\n    text = text.strip()\n    \n    return text\n\ndocs = df['text'].apply(preprocess).to_list()\nprint('Data preprocessing complete.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:24:54.120622Z","iopub.execute_input":"2025-06-29T01:24:54.120917Z","iopub.status.idle":"2025-06-29T01:24:55.135705Z","shell.execute_reply.started":"2025-06-29T01:24:54.120894Z","shell.execute_reply":"2025-06-29T01:24:55.134793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Creating the Text Embeddings","metadata":{}},{"cell_type":"markdown","source":"Sentence embeddings are numerical representations of text that capture semantic meaning. We use a pretrained SentenceTransformers model to generate the embeddings of our data. \n\n*   **Embedding Model:** We use the \"avsolatorio/GIST-small-Embedding-v0\" model (via SentenceTransformers), one of the highest scoring semantic text embedding models of the <100m parameter range on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), particularly on clustering-related benchmarks. This model has a 33.4 million parameter size, which is lightweight enough to load quickly in a Kaggle or Colab runtime (with the GPU runtime enabled).\n*   **Encoding:** The `embedding_model.encode()` function generates embeddings for the `docs` (the list of paper texts).\n*   **Device Usage:** `device=device` ensures that the embedding generation uses the available hardware acceleration (GPU or CPU).","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\nembedding_model_name = \"avsolatorio/GIST-small-Embedding-v0\"\n\nembedding_model = SentenceTransformer(embedding_model_name,\n                                      trust_remote_code=True, \n                                      device=device,\n                                     )\nif not load_embeddings_from_storage:\n    print(\"Creating embeddings...\")\n    embeddings = embedding_model.encode(docs, show_progress_bar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:24:55.144035Z","iopub.execute_input":"2025-06-29T01:24:55.14422Z","iopub.status.idle":"2025-06-29T01:25:38.800297Z","shell.execute_reply.started":"2025-06-29T01:24:55.144204Z","shell.execute_reply":"2025-06-29T01:25:38.799565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Creating the Topic Model","metadata":{}},{"cell_type":"markdown","source":"We are now ready to initialize a BERTopic model. \n\n#### Parameters\n- Now that the embeddings are generated, CountVectorizer is used to remove English stop words (words like \"for\", \"and\", \" \"to\", etc.).\n- The previously defined embedding model is used.\n- N-gram range of (1,3) is used to capture single words, bi-grams (pairs of words such as \"Computer Vision\" and \"Reinforcement Learning\"), and tri-grams (terms like \"Large Language Models\" \"Time Series Forecasting\", etc.)\n- Verbose mode enabled for training progress updates.\n- We set a minimum topic size of 35, so that smaller topics with less than 35 document examples do not get clustered. This prevents noise in the results at the expense of not capturing the entire breadth of topics in the dataset. \n","metadata":{}},{"cell_type":"code","source":"vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\nprint(\"Creating new model...\")\n\ntopic_model = BERTopic(\n    verbose=True,\n    embedding_model=embedding_model,\n    vectorizer_model=vectorizer_model,\n    n_gram_range=(1, 3),\n    min_topic_size=30,\n)\n\ntopics, probs = topic_model.fit_transform(docs, embeddings)\nprint(\"The topic model has been created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:25:38.801241Z","iopub.execute_input":"2025-06-29T01:25:38.801479Z","iopub.status.idle":"2025-06-29T01:25:53.353264Z","shell.execute_reply.started":"2025-06-29T01:25:38.801458Z","shell.execute_reply":"2025-06-29T01:25:53.352489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following table shows the model output. We will explore this further in the sections below. \n- **Topic**: The topic ID. Note that the Topic ID \"-1\" represents the outliers (documents that were not clustered into any specific topic). \n- **Count**: The number of documents that were clustered into the topic.\n- **Representation**: The list of the top words that represent the topic.\n- **Representative_Docs**: A sample of representative documents for the topic.","metadata":{}},{"cell_type":"code","source":"topic_info = topic_model.get_topic_info()\ntopic_info.head(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:25:53.354042Z","iopub.execute_input":"2025-06-29T01:25:53.354348Z","iopub.status.idle":"2025-06-29T01:25:53.382676Z","shell.execute_reply.started":"2025-06-29T01:25:53.354324Z","shell.execute_reply":"2025-06-29T01:25:53.381779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Evaluation Metric Scores ","metadata":{}},{"cell_type":"markdown","source":"Below, we use standard evaluation metrics for topic modeling. We use the OCTIS (Optimizing and Comparing Topic Models is Simple) library to calculate the NPMI topic coherence score and the topic diversity score. \nThe coherence score calculation can take a few minutes, even when using multithreading.\n\nThe NPMI coherence score of **~0.22** suggests that our topics are reasonably interpretable, meaning the words within each topic tend to be semantically related. Our diversity score of **~0.73** suggests that our model is decently capturing different themes within our data, although an ideal score would be above 80. This is expected, however, since we have limited our topic size to a minimum of 35 representative documents (skipping many smaller topics), and our dataset is supposed to have common themes (topics related to machine learning).  \n\nWhile it is good to have these evaluation metrics, determining the relevance of clustered topics is an inherently subjective process, so it is difficult and impractical to obtain a clear picture of the topic model's quality while only relying on purely quantitative metrics. ","metadata":{}},{"cell_type":"code","source":"topics_dict  = topic_model.get_topics()                 \ntopic_words  = [\n    [w for w, _ in words[:10]]                 # top-10 words\n    for tid, words in topics_dict.items()\n    if tid != -1                               # skip outliers\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:25:53.385027Z","iopub.execute_input":"2025-06-29T01:25:53.385255Z","iopub.status.idle":"2025-06-29T01:25:53.39823Z","shell.execute_reply.started":"2025-06-29T01:25:53.385235Z","shell.execute_reply":"2025-06-29T01:25:53.397451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer = topic_model.vectorizer_model # the CountVectorizer that c-TF-IDF used\nanalyzer   = vectorizer.build_analyzer()  # includes lowercase, n-grams, stop-words …\n\ntokenized_docs = [analyzer(d) for d in docs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:25:53.399357Z","iopub.execute_input":"2025-06-29T01:25:53.399546Z","iopub.status.idle":"2025-06-29T01:25:54.962272Z","shell.execute_reply.started":"2025-06-29T01:25:53.399529Z","shell.execute_reply":"2025-06-29T01:25:54.961574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from octis.evaluation_metrics.coherence_metrics import Coherence\nfrom octis.evaluation_metrics.diversity_metrics import TopicDiversity\n\noctis_format = {\"topics\": topic_words}\n\ncoh = Coherence(texts=tokenized_docs, topk=5, processes=4).score(octis_format)\nprint(f\"c_npmi={coh:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:25:54.963648Z","iopub.execute_input":"2025-06-29T01:25:54.963863Z","iopub.status.idle":"2025-06-29T01:26:14.776349Z","shell.execute_reply.started":"2025-06-29T01:25:54.963845Z","shell.execute_reply":"2025-06-29T01:26:14.775491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"div = TopicDiversity(topk=10).score(octis_format)\nprint(f\"diversity={div:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:14.777399Z","iopub.execute_input":"2025-06-29T01:26:14.77773Z","iopub.status.idle":"2025-06-29T01:26:14.783485Z","shell.execute_reply.started":"2025-06-29T01:26:14.777704Z","shell.execute_reply":"2025-06-29T01:26:14.782635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Exploring and Visualizing the Results","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Creating New Topic Labels and Summaries using Large Language Models","metadata":{}},{"cell_type":"markdown","source":"As seen in the table above, the names of our topics use a simple topic representation model: the topic ID number, followed by the top representative words, separated with underscores.\nWe can use a Large Language Model (LLM) to create a more descriptive label. Additionally, we can use it to generate a short summary for each topic. \n\nAt the moment, Google offers an experimental version of Gemini Flash 2.0-Lite at only 0.0075 USD per million input tokens and 30 cents per million output tokens. Additionally, it offers 15 requests per minute (RPM) and 1500 free requests per day. It is distinguished for being small, fast, and capable of high quality outputs for non-reasoning tasks. Since the model has a large context window, we do not really need to batch our requests, but our code below processes our topics list in two halves, just in case (some models may have degraded performance when the included context is too large).\n\nWe use LiteLLM, a library that supports API calls to many LLM providers using a common interface. With this library, we can swap out our chosen model for a new one in the future without having to rewrite most of the code.\n\nNote: You must set up your own Gemini API key. After creating your key through Google AI Studio, go to the \"Add-ons\" > \"Secrets\" > \"Add Secret\", then create a secret with the name \"GEMINI_API_KEY\" and paste your unique API token. ","metadata":{}},{"cell_type":"code","source":"import os, math, json, time\nfrom litellm import completion\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\nos.environ[\"GEMINI_API_KEY\"] = api_key\n\ndef prompt_builder(df_chunk):\n    # concise system instruction\n    system_msg = \"\"\"You are an ML researcher who names Machine Learning research topic clusters \n    generated by BERTopic. \n    For each topic you receive, return a JSON object with:\n          id      : integer   (the topic ID)\n          Label   : ≤ 6 words (concise title)\n          Summary : ≤ 25 words (short description of the topic)\n        Each topic cluster you receive includes its name and representation (top words and phrases found in the cluster).\n        Respond with a JSON list only—no extra text.\n    \"\"\".strip()\n\n    # build a small TSV block the model can read\n    rows = [\n        f\"\\t{r.Name}\\t{r.Representation}\"\n        for _, r in df_chunk.iterrows()\n    ]\n    user_msg = (\n        \"Columns: id, size, placeholder_label, top_keywords\\n\"\n        \"```text\\n\" + \"\\n\".join(rows) + \"\\n```\"\n    )\n\n    return [\n        {\"role\": \"system\", \"content\": system_msg},\n        {\"role\": \"user\",   \"content\": user_msg}\n    ]\n\n# split the DataFrame\nmidpoint = math.ceil(len(topic_info) / 2)\n\nfirst_half  = topic_info.iloc[:midpoint]\nsecond_half = topic_info.iloc[midpoint:]\n\n# build prompts and call LLM\ndef call_gemini(df_part):\n    messages = prompt_builder(df_part)      \n    resp = completion(\n        model=\"gemini/gemini-2.0-flash\",\n        messages=messages,\n        response_format={\"type\": \"json_object\"},\n        temperature=0.3,                         \n        max_tokens=100000,                    \n    )\n    raw_json = resp.choices[0].message.content   \n    return json.loads(raw_json)                  \n\nresults_1 = call_gemini(first_half)\nresults_2 = call_gemini(second_half)\n\n# --- 3. stitch back together --------------------------------------------------\nall_results = results_1 + results_2\nall_results.sort(key=lambda d: d[\"id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:14.78438Z","iopub.execute_input":"2025-06-29T01:26:14.784592Z","iopub.status.idle":"2025-06-29T01:26:31.199682Z","shell.execute_reply.started":"2025-06-29T01:26:14.784573Z","shell.execute_reply":"2025-06-29T01:26:31.198895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model was instructed to generate its output in structured JSON, which can be easily parsed into a dataframe and merged with the dataframe generated by `get_topic_info()`.","metadata":{}},{"cell_type":"code","source":"import json\n\nlabel_df = pd.DataFrame(all_results)                \nlabel_df.rename(columns={\"id\": \"Topic\"}, inplace=True)\n\n# apply to BERTopic \n# merge so we keep original ordering and any extra columns\ntopic_info = topic_model.get_topic_info()\ntopic_info = topic_info.merge(label_df, on=\"Topic\", how=\"left\")\n\n# use the “label” column as custom topic names\ncustom_labels = topic_info.set_index(\"Topic\")[\"Label\"].to_dict()\ntopic_model.set_topic_labels(custom_labels)\n\n# store the summaries for later display\ntopic_summaries = topic_info.set_index(\"Topic\")[\"Summary\"].to_dict()\nprint('Topic labels and summaries have been added to the topic model.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.200625Z","iopub.execute_input":"2025-06-29T01:26:31.200975Z","iopub.status.idle":"2025-06-29T01:26:31.229196Z","shell.execute_reply.started":"2025-06-29T01:26:31.200938Z","shell.execute_reply":"2025-06-29T01:26:31.228364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2 Top Topics Table","metadata":{}},{"cell_type":"markdown","source":"The table below shows the top 30 most frequent topics, including the new summaries and labels, with the outliers topic (Topic -1) at the top. You can change the value of `head(30)` to see more or less of the top topics. You can also change `head(30)` to `tail(30)` to see the bottom 30 topics as well. ","metadata":{}},{"cell_type":"code","source":"topic_info[[\"Topic\", \"Count\", \"Name\", \"Label\", \"Summary\"]].head(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.230067Z","iopub.execute_input":"2025-06-29T01:26:31.230366Z","iopub.status.idle":"2025-06-29T01:26:31.252209Z","shell.execute_reply.started":"2025-06-29T01:26:31.230335Z","shell.execute_reply":"2025-06-29T01:26:31.251375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.3 Intertopic Distance Map","metadata":{}},{"cell_type":"markdown","source":"Our first visualization shows the relationships between topics in a 2D space. Topics that are closer together are semantically more similar. Since we set the labels generated by the LLM as our custom topic labels, they can now be used in the visualizations.\n\n\nThe map allows for interactive exploration. Upon hovering over the circles, the topic names and sizes are shown. Any area can be selected to zoom in for closer inspection. The size of each circle corresponds to the topic's prevalence in the dataset, making it easy to identify dominant themes.\n\nThis visualization provides a clear and intuitive overview of topic relationships, and allows us to see which topics have enough overlap to be merged if we want to trim down our number of topics even further. To see the topic labels, sizes and IDs, simply hover over each circle with the mouse. The slider can be used to highlight a specific topic.\n\n","metadata":{}},{"cell_type":"code","source":"fig = topic_model.visualize_topics(custom_labels=True)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.252923Z","iopub.execute_input":"2025-06-29T01:26:31.253122Z","iopub.status.idle":"2025-06-29T01:26:31.401767Z","shell.execute_reply.started":"2025-06-29T01:26:31.253105Z","shell.execute_reply":"2025-06-29T01:26:31.401004Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.4 Topic Word Scores Bar Chart","metadata":{}},{"cell_type":"markdown","source":"This bar chart visualization highlights the top words associated with each topic identified by the BERTopic model. The topics are represented by their most representative terms, ranked by relevance scores. The length of the bars corresponds to the importance of each word in defining the topic. Note how the Reinforcement Learning topic is not just represented by 'reinforcement' and 'learning,' but also by related concepts such as 'reward' (the feedback signal that the algorithm seeks to maximize over time) and 'policy' (the strategy or mapping from states to actions that the algorithm learns).\nThis demonstrates how the BERTopic model’s underlying embeddings effectively capture the semantic relationships between terms and concepts","metadata":{}},{"cell_type":"code","source":"fig = topic_model.visualize_barchart(custom_labels=True, height=300, width=385)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.402732Z","iopub.execute_input":"2025-06-29T01:26:31.402995Z","iopub.status.idle":"2025-06-29T01:26:31.469642Z","shell.execute_reply.started":"2025-06-29T01:26:31.402974Z","shell.execute_reply":"2025-06-29T01:26:31.468967Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.5 Topic Similarity Heatmap","metadata":{}},{"cell_type":"markdown","source":"The similarity matrix heatmap offers a good way to inspect the relationships between pairs of topics. Each row and column corresponds to a particular topic, and the color of each cell reflects the degree of semantic similarity between those two topics. Darker cells along the diagonal indicate higher self-similarity (a topic compared to itself), while off-diagonal cells reveal how related (or unrelated) different topics are.\n\nFrom the heatmap, you can see which topics tend to cluster together. Topics that share conceptual ground, such as “Bandit Algorithms” and “Reinforcement Learning Policies”, appear in regions of higher similarity, suggesting that the language used to describe them overlaps significantly. Conversely, less closely related topics have lower similarity scores, appearing in lighter-colored cells. Note that the results below might differ from the examples described, due to the stochastic nature of the topic model and the LLM outputs.","metadata":{}},{"cell_type":"code","source":"fig = topic_model.visualize_heatmap(custom_labels=True, top_n_topics=25)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.470408Z","iopub.execute_input":"2025-06-29T01:26:31.470706Z","iopub.status.idle":"2025-06-29T01:26:31.514928Z","shell.execute_reply.started":"2025-06-29T01:26:31.470677Z","shell.execute_reply":"2025-06-29T01:26:31.514121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Document-Level Visualizations","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Visualize Documents with Hoverable Titles","metadata":{}},{"cell_type":"markdown","source":"The following visualization is a scatter plot where each point represents a document. This scatter plot visualizes the distribution of documents and their assigned topics in a two-dimensional space. Each point represents a document, and points are colored according to their topic. Labels indicate the general area of the plot where a particular topic is most prominent, showing how the BERTopic model clusters semantically similar documents together.\n\nThe documents are colored by their assigned topic. Hovering over a point shows the document's title. \n\nThe clustering and separation of points indicate the effectiveness of the topic modeling process, with clear groupings suggesting coherent topic definitions.\n","metadata":{}},{"cell_type":"code","source":"fig = topic_model.visualize_documents(df[\"title\"], \n                           title=\"Documents and Topics\",\n                           embeddings=embeddings,\n                           custom_labels=True, \n                           hide_annotations=True, \n                           topics=topics)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:31.515737Z","iopub.execute_input":"2025-06-29T01:26:31.516169Z","iopub.status.idle":"2025-06-29T01:26:35.142214Z","shell.execute_reply.started":"2025-06-29T01:26:31.516138Z","shell.execute_reply":"2025-06-29T01:26:35.140923Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.2 Documents with Labeled Topics","metadata":{}},{"cell_type":"markdown","source":"This version of the visualization shows the document clusters with their respective topic labels. In order to make space for the labels, only the top 55 document clusters are used for the `topics` parameter.","metadata":{}},{"cell_type":"code","source":"top_topics = topics[:55]\nfig = topic_model.visualize_documents(docs, \n                           title=\"Documents and Topics\",\n                           embeddings=embeddings, \n                           hide_document_hover=True, \n                           custom_labels=True, \n                           topics=top_topics)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:35.143179Z","iopub.execute_input":"2025-06-29T01:26:35.143406Z","iopub.status.idle":"2025-06-29T01:26:38.553153Z","shell.execute_reply.started":"2025-06-29T01:26:35.143386Z","shell.execute_reply":"2025-06-29T01:26:38.552228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Topic Search","metadata":{}},{"cell_type":"markdown","source":"Below, we put our topic modeling to use with a search engine for our data, allowing for filtering by topic. The engine uses a simple cosine similarity algorithm and leverages the same embedding model that was used for the topic model.\n\n* Enter you search query and click the search button to search accross all topics.\n* Click on a topic from the list to choose a filter.\n* Click the clear button to remove your input text and your filtered topic.\n\nTry the following search queries with no filter selected: \"rag\", \"image segment\", \"cluster\", and \"TTS\".","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML, clear_output\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef search_papers(\n    query,\n    topic_model,\n    df,\n    embeddings,\n    embedder,\n    top_k=15,\n    topic_filter=None):\n    # 1. Encode the query. Reshape to (1, n_features) for cosine_similarity.\n    query_emb = embedder.encode([query], show_progress_bar=False)\n\n    # 2. Get all document topics from the model.\n    doc_topics = np.array(topic_model.topics_)\n    \n    # 3. Apply the topic filter before calculating similarity.\n    search_indices = np.arange(len(embeddings))\n    search_embeddings = embeddings\n\n    if topic_filter:\n        mask = np.isin(doc_topics, topic_filter)\n        search_indices = np.where(mask)[0]\n        \n        if len(search_indices) == 0:\n            return []\n            \n        search_embeddings = embeddings[search_indices]\n\n    # 4. Calculate cosine similarity against the (potentially filtered) embeddings.\n    similarities = cosine_similarity(query_emb, search_embeddings)[0]\n\n    # 5. Get the top-k indices from the filtered results.\n    num_results = min(top_k, len(similarities))\n    if num_results == 0:\n        return []\n\n    top_filtered_indices = np.argpartition(similarities, -num_results)[-num_results:]\n    top_filtered_indices = top_filtered_indices[np.argsort(similarities[top_filtered_indices])[::-1]]\n\n    # 6. Build the results list.\n    results = []\n    for idx in top_filtered_indices:\n        original_idx = search_indices[idx]\n        topic_id = doc_topics[original_idx]\n        try:\n            if topic_id == \"-1\":\n                topic_label = \"Outlier Topics\"\n            else:\n                topic_label = topic_info.loc[topic_info[\"Topic\"] == topic_id, \"Label\"].iloc[0]\n                topic_summary = topic_info.loc[topic_info[\"Topic\"] == topic_id, \"Summary\"].iloc[0]\n        except IndexError:\n            topic_label = f\"Topic {topic_id}\"\n            topic_summary = \"\"\n\n        results.append(\n            {\n                \"title\": df.at[original_idx, \"title\"],\n                \"summary\": df.at[original_idx, \"summary\"],\n                \"topic\": topic_label,\n                \"topic_summary\": topic_summary,\n                \"topic_id\": int(topic_id),\n                \"similarity\": float(similarities[idx]),\n            }\n        )\n    return results\n\n\ndef create_search_interface(\n    topic_model,\n    df,\n    embeddings,\n    embedder,\n    topic_info):\n    \"\"\"\n    Creates and displays a search interface in a Jupyter environment.\n    \"\"\"\n\n    # Widgets\n    search_box = widgets.Text(\n        placeholder=\"Enter search query…\",\n        description=\"Search:\",\n        layout=widgets.Layout(width=\"50%\"),\n    )\n\n    topic_options = [\n        (row[\"Name\"], int(row[\"Topic\"]))\n        for _, row in topic_info[topic_info[\"Topic\"] != -1].iterrows()\n    ]\n\n    topic_dropdown = widgets.SelectMultiple(\n        options=topic_options,\n        description=\"Filter topics:\",\n        layout=widgets.Layout(width=\"50%\", height=\"200px\"),\n    )\n\n    results_out = widgets.Output()\n    \n    # Event Handlers\n    def run_search(_):\n        with results_out:\n            clear_output()\n            if not search_box.value.strip():\n                display(HTML(\"<em>Please enter a search query.</em>\"))\n                return\n\n            topic_filter = list(topic_dropdown.value) or None\n            \n            # Call the refined search function\n            hits = search_papers(\n                search_box.value,\n                topic_model,\n                df,\n                embeddings,\n                embedder,\n                top_k=15,\n                topic_filter=topic_filter,\n            )\n            \n            if not hits:\n                display(HTML(\"<em>No results found.</em>\"))\n                return\n\n            for i, hit in enumerate(hits, 1):\n                html = f\"\"\"\n                <div style=\"margin:12px 0; padding:12px; border:1px solid #e0e0e0; border-radius: 8px; background-color: #f9f9f9;\">\n                    <h3 style=\"margin-top:0;\">{i}. {hit['title']}</h3>\n                    <p><b>Topic:</b> {hit['topic']} (ID: {hit['topic_id']})</p>\n                    <p><b>Similarity:</b> {hit['similarity']:.3f}</p>\n                    <p><b>Summary:</b> {hit['summary'][:500]}…</p>\n                    <p><b>Topic Summary:</b> {hit['topic_summary'][:500]}.</p>\n\n                </div>\"\"\"\n                display(HTML(html))\n\n    def clear_form(_):\n        search_box.value = \"\"\n        topic_dropdown.value = ()\n        with results_out:\n            clear_output()\n\n    # Assemble UI\n    search_btn = widgets.Button(description=\"Search\", button_style='primary')\n    clear_btn = widgets.Button(description=\"Clear\")\n    search_btn.on_click(run_search)\n    clear_btn.on_click(clear_form)\n\n    ui = widgets.VBox(\n        [\n            search_box,\n            topic_dropdown,\n            widgets.HBox([search_btn, clear_btn]),\n            results_out,\n        ]\n    )\n    display(ui)\n\ncreate_search_interface(topic_model, df, embeddings, embedding_model, topic_info)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T01:26:38.554069Z","iopub.execute_input":"2025-06-29T01:26:38.554318Z","iopub.status.idle":"2025-06-29T01:26:38.587624Z","shell.execute_reply.started":"2025-06-29T01:26:38.554294Z","shell.execute_reply":"2025-06-29T01:26:38.586648Z"}},"outputs":[],"execution_count":null}]}